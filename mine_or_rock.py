# -*- coding: utf-8 -*-
"""Mine_or_Rock.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bgfp4_B9qq1CoOsYTy3_4u4_Qy8qsFfF
"""

import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix,classification_report
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

sonar_df=pd.read_csv('/content/drive/MyDrive/mine_vs_rock.csv',header=None)
sonar_df.head()

sonar_df.isnull()

sonar_df.duplicated()

sonar_df.describe()

df=sonar_df
df0 = df[df[60] == 'R']
df1 = df[df[60] == 'M']

print("Number of samples in:")
print("Class label R - ", len(df0))
print("Class label M - ", len(df1))

df0 = df0.sample(len(df1), replace = True)

print('\nAfter resampling - ')
print("Number of samples in:")
print("Class label R - ", len(df0))
print("Class label M - ", len(df1))

df = df1.append(df0)
print('Total number of samples - ', len(df))

np.shape(df)

df[60].value_counts().plot(kind='bar')

fig = plt.figure(figsize = (15,30))
ax = fig.gca()
df.hist(ax = ax)

plt.figure(figsize=(8,5))
plt.plot(df[df[60] == 'R'].values[0][:-1], label='Rock', color='black')
plt.plot(df[df[60] == 'M'].values[0][:-1], label='Mine', color='gray', linestyle='--')
plt.legend()
plt.title('Example of both classes')
plt.xlabel('Frequency bin')
plt.ylabel('Power spectral density (normalized)')
plt.tight_layout()
plt.show()

X = df.drop(columns=60,axis=1)
Y = df[60]
print(X)
print(Y)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train,Y_train)

test_pred = lr.predict(X_test)
lra = accuracy_score(test_pred,Y_test)
print(lra)

cm = metrics.confusion_matrix(test_pred,Y_test)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

pd.crosstab(Y_test, test_pred, colnames=['Predicted'], margins=True)

print(classification_report(test_pred, Y_test))

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, Y_train)

test_pred = knn.predict(X_test)
knna = accuracy_score(test_pred,Y_test)
print(knna)

cm = metrics.confusion_matrix(test_pred,Y_test)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

pd.crosstab(Y_test, test_pred, colnames=['Predicted'], margins=True)

print(classification_report(test_pred, Y_test))

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(criterion = 'entropy')
dt.fit(X_train,Y_train)

test_pred = dt.predict(X_test)
dta = accuracy_score(test_pred,Y_test)
print(dta)

cm = metrics.confusion_matrix(test_pred, Y_test)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

pd.crosstab(Y_test, test_pred, colnames=['Predicted'], margins=True)

print(classification_report(test_pred, Y_test))

from sklearn.svm import SVC
svm = SVC(kernel = 'rbf')
svm.fit(X_train,Y_train)

test_pred = svm.predict(X_test)
svma = accuracy_score(test_pred,Y_test)
print(svma)

cm = metrics.confusion_matrix(test_pred, Y_test)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

print(classification_report(test_pred, Y_test))

plt.figure(figsize= (10,6))
acc = [lra,knna,dta,svma]
name = ['LogReg','KNN','DTree','SVM']
plt.title('Cumulative Accuracy of different models:', fontsize = 20)
plt.xlabel('Acuracy', fontsize = 15)
plt.bar(name,acc)
plt.show()

input_data = (0.0201,0.0376,0.0438,0.0207,0.0954,0.0906,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.0660,0.2273,0.3100,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.5550,0.6711,0.6415,0.7104,0.8080,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.0510,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.0180,0.0084,0.0090,0.0032)
input_data_np_array = np.asarray(input_data)
reshaped_input = input_data_np_array.reshape(1,-1)
prediction = knn.predict(reshaped_input)

if prediction[0] == 'R':
  print('The object is a Rock')
else:
  print('The object is a Mine')